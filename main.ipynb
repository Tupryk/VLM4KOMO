{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM for KOMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import robotic as ry\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import grasping_within_komo_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device Name: {torch.cuda.get_device_name(device)}\" if device.type == \"cuda\" else \"Using cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = ry.Config()\n",
    "C.addFile(ry.raiPath('scenarios/pandaSingle.g'))\n",
    "\n",
    "C.addFrame('box') \\\n",
    "    .setPosition([-.25,.1,1.]) \\\n",
    "    .setShape(ry.ST.ssBox, size=[.06,.06,.06,.005]) \\\n",
    "    .setColor([1,.5,0]) \\\n",
    "    .setContact(1)\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a picture of the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = ry.BotOp(C, False)\n",
    "bot.home(C)\n",
    "bot.gripperMove(ry._left)\n",
    "\n",
    "rgb, depth = bot.getImageAndDepth(\"cameraWrist\")\n",
    "\n",
    "image_path = \"./scene_image.jpg\"\n",
    "plt.imsave(image_path, rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your model (VLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt the model (for KOMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompting import prompt_qwen\n",
    "\n",
    "task_description = \"Put the blob in the bin.\"\n",
    "\n",
    "komo_definition = prompt_qwen(model, processor, device, task_description, image_path)\n",
    "print(komo_definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the komo problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ time: 0.000313, evals: 6, done: 1, feasible: 1, sos: 0, f: 0, ineq: 0, eq: 0.00256402 }\n"
     ]
    }
   ],
   "source": [
    "komo: ry.KOMO = None\n",
    "exec(komo_definition)\n",
    "ret = ry.NLP_Solver(komo.nlp(), verbose=0).solve()\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = komo.getPath()\n",
    "\n",
    "for q in qs:\n",
    "    C.setJointState(q)\n",
    "    C.view()\n",
    "    time.sleep(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify phases where we grasp\n",
    "switch_indices = grasping_within_komo_definition(komo_definition)\n",
    "\n",
    "# Execute on robot\n",
    "prev_s = 0\n",
    "for i, s in enumerate(switch_indices):\n",
    "    \n",
    "    # Moving\n",
    "    bot.moveAutoTimed(qs[prev_s:s])\n",
    "    while bot.getTimeToEnd() > 0:\n",
    "        bot.sync(C)\n",
    "\n",
    "    # Grasping\n",
    "    if i % 2 == 0:\n",
    "        bot.gripperClose(ry._left)\n",
    "    else:\n",
    "        bot.gripperMove(ry._left)\n",
    "        \n",
    "    prev_s = s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
